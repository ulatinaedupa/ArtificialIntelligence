{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- **Attention Mechanism**: Transformers use a process called attention that allows them to focus on different parts of the input data. This means they can understand which words in a sentence are most important, helping them grasp context and meaning more effectively.\n",
       "\n",
       "- **Parallel Processing**: Unlike older models that read text one word at a time, transformers can process multiple words at once. This speeds up their ability to understand and generate language, making them faster and more efficient.\n",
       "\n",
       "- **Layers and Learning**: Transformers consist of multiple layers that refine their understanding of the data. Each layer learns to recognize different patterns or features, enabling them to improve their predictions and generate more coherent and relevant responses."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model='gpt-4o-mini')\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a research assistant'),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "basic_chain = prompt | llm | output_parser\n",
    "\n",
    "output = basic_chain.invoke(input='Write a 3 bullet point summary about how transformers work. Simplify to non-technical people but keep the main bits of information.')\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a draft of a research report using chains in langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_SYS_MSG = \"\"\"\n",
    "You are a research assistant and a scientific writer.\n",
    "You take in requests about tpics and write organized research reprts on those topics.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', WRITER_SYS_MSG),\n",
    "    ('human', 'Write an organized research report about this topic:\\n\\n{topic}.')\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "writer_chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Understanding Transformers: A Guide for Non-AI Researchers\n",
       "\n",
       "## Introduction\n",
       "Transformers are a type of neural network architecture that has revolutionized the field of artificial intelligence (AI), particularly in natural language processing (NLP). Introduced in a 2017 paper titled \"Attention is All You Need\" by Vaswani et al., transformers have become the backbone of many state-of-the-art AI models, including BERT, GPT, and T5. This report aims to explain the fundamental concepts of transformers in a way that is accessible to non-AI researchers.\n",
       "\n",
       "## 1. The Need for Transformers\n",
       "Before transformers, traditional models for processing sequences of data (like sentences) relied heavily on recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). These models processed data sequentially, which made them slow and less effective at capturing long-range dependencies in text. Transformers were developed to address these limitations by allowing for parallel processing and better handling of context.\n",
       "\n",
       "## 2. Key Components of Transformers\n",
       "Transformers consist of several key components that work together to process and generate data:\n",
       "\n",
       "### 2.1. Input Representation\n",
       "Transformers take input data, such as words in a sentence, and convert them into numerical representations called embeddings. Each word is represented as a vector in a high-dimensional space, capturing its meaning and context.\n",
       "\n",
       "### 2.2. Attention Mechanism\n",
       "The core innovation of transformers is the attention mechanism. This allows the model to weigh the importance of different words in a sentence when making predictions. For example, in the sentence \"The cat sat on the mat,\" the model can focus more on \"cat\" and \"sat\" when predicting the next word, rather than treating all words equally.\n",
       "\n",
       "#### 2.2.1. Self-Attention\n",
       "Self-attention is a specific type of attention where the model looks at all the words in a sentence to determine their relationships. Each word attends to every other word, allowing the model to capture context effectively. This is done through three vectors: Query (Q), Key (K), and Value (V). The attention score is calculated using these vectors, which helps the model decide how much focus to give to each word.\n",
       "\n",
       "### 2.3. Multi-Head Attention\n",
       "Instead of using a single attention mechanism, transformers employ multiple attention heads. Each head learns different aspects of the relationships between words, allowing the model to capture a richer understanding of the input data.\n",
       "\n",
       "### 2.4. Feed-Forward Neural Networks\n",
       "After the attention mechanism, the output is passed through a feed-forward neural network. This component processes the information further, applying non-linear transformations to enhance the model's ability to learn complex patterns.\n",
       "\n",
       "### 2.5. Positional Encoding\n",
       "Since transformers do not process data sequentially, they need a way to understand the order of words. Positional encoding is added to the input embeddings to provide information about the position of each word in the sequence.\n",
       "\n",
       "## 3. Architecture of Transformers\n",
       "The transformer architecture consists of an encoder and a decoder:\n",
       "\n",
       "### 3.1. Encoder\n",
       "The encoder is responsible for processing the input data. It consists of multiple layers, each containing a multi-head self-attention mechanism followed by a feed-forward neural network. The output of the encoder is a set of context-aware embeddings.\n",
       "\n",
       "### 3.2. Decoder\n",
       "The decoder generates the output sequence (e.g., a translated sentence). It also consists of multiple layers, but it includes an additional attention mechanism that allows it to focus on the encoder's output while generating each word.\n",
       "\n",
       "## 4. Training Transformers\n",
       "Transformers are trained using large datasets and a process called supervised learning. During training, the model learns to predict the next word in a sentence based on the previous words. This is done by minimizing the difference between the predicted and actual words, using a technique called backpropagation.\n",
       "\n",
       "## 5. Applications of Transformers\n",
       "Transformers have a wide range of applications, including:\n",
       "\n",
       "- **Natural Language Processing**: Language translation, sentiment analysis, and text summarization.\n",
       "- **Computer Vision**: Image classification and object detection.\n",
       "- **Speech Recognition**: Converting spoken language into text.\n",
       "\n",
       "## Conclusion\n",
       "Transformers represent a significant advancement in AI, particularly in how machines understand and generate human language. By leveraging the attention mechanism and parallel processing, transformers can capture complex relationships in data more effectively than previous models. As AI continues to evolve, understanding the basics of transformers will be essential for researchers and practitioners across various fields.\n",
       "\n",
       "## References\n",
       "- Vaswani, A., Shard, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems (NeurIPS).\n",
       "- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems (NeurIPS)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = writer_chain.invoke({'topic': 'How do transformers work for non AI researchers?'})\n",
    "\n",
    "Markdown(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Feedback on \"Understanding Transformers: A Guide for Non-AI Researchers\"\n",
       "\n",
       "1. **Clarity and Accessibility**: The report does a commendable job of breaking down complex concepts into understandable segments for non-AI researchers. The use of simple language and clear explanations helps demystify the transformer architecture.\n",
       "\n",
       "2. **Structure and Organization**: The report is well-structured, with a logical flow from the introduction to the conclusion. Each section builds on the previous one, making it easy for readers to follow the progression of ideas.\n",
       "\n",
       "3. **Depth of Content**: While the report covers the fundamental components of transformers effectively, it could benefit from a few more practical examples or analogies to further illustrate how these components work together in real-world applications. This would enhance understanding for readers unfamiliar with AI.\n",
       "\n",
       "4. **Applications Section**: The applications of transformers are briefly mentioned, but this section could be expanded to include specific examples or case studies. Highlighting notable transformer-based models in various fields would provide readers with a clearer picture of their impact.\n",
       "\n",
       "5. **References and Citations**: The references provided are relevant and foundational to the topic. However, including a few more recent studies or reviews (post-2020) would strengthen the report by showcasing the ongoing developments in transformer research and applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REVIEWER_SYS_MSG = \"\"\"\n",
    "You are a reviewer for research reports. You take in research reports and provide feecback on them.\n",
    "\"\"\"\n",
    "\n",
    "prompt_reviewer = ChatPromptTemplate.from_messages([\n",
    "    ('system', REVIEWER_SYS_MSG),\n",
    "    ('human', 'Provide feedback on this research report:\\n\\n{report}. As 5 concise bullet points.')\n",
    "])\n",
    "\n",
    "llm_reviewer = ChatOpenAI(model='gpt-4o-mini', temperature=0.2)\n",
    "\n",
    "review_chain = prompt_reviewer | llm_reviewer | output_parser\n",
    "\n",
    "feedback_output = review_chain.invoke({'report': output})\n",
    "\n",
    "Markdown(feedback_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Understanding Transformers: A Guide for Non-AI Researchers\n",
       "\n",
       "## Introduction\n",
       "Transformers are a groundbreaking neural network architecture that has transformed the field of artificial intelligence (AI), especially in natural language processing (NLP). Introduced in the seminal 2017 paper \"Attention is All You Need\" by Vaswani et al., transformers have become the backbone of many state-of-the-art AI models, including BERT, GPT, and T5. This report aims to explain the fundamental concepts of transformers in an accessible manner for non-AI researchers, providing clarity on their significance and applications.\n",
       "\n",
       "## 1. The Need for Transformers\n",
       "Prior to the advent of transformers, traditional models for processing sequential data, such as sentences, relied heavily on recurrent neural networks (RNNs) and long short-term memory networks (LSTMs). These models processed data sequentially, which often resulted in slower performance and challenges in capturing long-range dependencies in text. Transformers were developed to overcome these limitations by enabling parallel processing and enhancing the model's ability to manage context effectively.\n",
       "\n",
       "## 2. Key Components of Transformers\n",
       "Transformers consist of several key components that work in unison to process and generate data:\n",
       "\n",
       "### 2.1. Input Representation\n",
       "Transformers convert input data, such as words in a sentence, into numerical representations called embeddings. Each word is represented as a vector in a high-dimensional space, which captures its meaning and context. This transformation is crucial for the model to understand the relationships between words.\n",
       "\n",
       "### 2.2. Attention Mechanism\n",
       "The attention mechanism is the core innovation of transformers. It allows the model to weigh the importance of different words in a sentence when making predictions. For instance, in the sentence \"The cat sat on the mat,\" the model can prioritize \"cat\" and \"sat\" when predicting the next word, rather than treating all words equally.\n",
       "\n",
       "#### 2.2.1. Self-Attention\n",
       "Self-attention is a specific type of attention where the model examines all the words in a sentence to determine their relationships. Each word attends to every other word, enabling the model to capture context effectively. This process involves three vectors: Query (Q), Key (K), and Value (V). The attention score is calculated using these vectors, guiding the model on how much focus to allocate to each word.\n",
       "\n",
       "### 2.3. Multi-Head Attention\n",
       "Transformers utilize multiple attention heads instead of a single attention mechanism. Each head learns different aspects of the relationships between words, allowing the model to develop a richer understanding of the input data. This diversity in attention enhances the model's ability to capture nuanced meanings.\n",
       "\n",
       "### 2.4. Feed-Forward Neural Networks\n",
       "Following the attention mechanism, the output is processed through a feed-forward neural network. This component applies non-linear transformations to the information, enhancing the model's capacity to learn complex patterns and relationships.\n",
       "\n",
       "### 2.5. Positional Encoding\n",
       "Since transformers do not process data sequentially, they require a method to understand the order of words. Positional encoding is added to the input embeddings to convey information about the position of each word in the sequence, ensuring that the model retains the necessary context.\n",
       "\n",
       "## 3. Architecture of Transformers\n",
       "The transformer architecture is composed of an encoder and a decoder:\n",
       "\n",
       "### 3.1. Encoder\n",
       "The encoder processes the input data and consists of multiple layers, each containing a multi-head self-attention mechanism followed by a feed-forward neural network. The output of the encoder is a set of context-aware embeddings that encapsulate the relationships between words.\n",
       "\n",
       "### 3.2. Decoder\n",
       "The decoder generates the output sequence (e.g., a translated sentence) and also consists of multiple layers. It includes an additional attention mechanism that allows it to focus on the encoder's output while generating each word, ensuring coherence and relevance in the output.\n",
       "\n",
       "## 4. Training Transformers\n",
       "Transformers are trained using large datasets through a process called supervised learning. During training, the model learns to predict the next word in a sentence based on the preceding words. This is achieved by minimizing the difference between the predicted and actual words, utilizing a technique known as backpropagation. The training process is resource-intensive but essential for developing highly effective models.\n",
       "\n",
       "## 5. Applications of Transformers\n",
       "Transformers have a wide array of applications across various fields, including:\n",
       "\n",
       "- **Natural Language Processing**: Language translation (e.g., Google Translate), sentiment analysis (e.g., analyzing customer reviews), and text summarization (e.g., summarizing news articles).\n",
       "- **Computer Vision**: Image classification (e.g., identifying objects in images) and object detection (e.g., detecting faces in photographs).\n",
       "- **Speech Recognition**: Converting spoken language into text (e.g., virtual assistants like Siri and Alexa).\n",
       "\n",
       "Notable transformer-based models, such as BERT for understanding context in text and GPT for generating human-like text, have demonstrated significant advancements in these applications.\n",
       "\n",
       "## Conclusion\n",
       "Transformers represent a monumental advancement in AI, particularly in how machines understand and generate human language. By leveraging the attention mechanism and parallel processing, transformers can capture complex relationships in data more effectively than previous models. As AI continues to evolve, a foundational understanding of transformers will be essential for researchers and practitioners across various fields.\n",
       "\n",
       "## References\n",
       "- Vaswani, A., Shard, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems (NeurIPS).\n",
       "- Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems (NeurIPS).\n",
       "- Zhang, Y., & Chen, Y. (2021). A Comprehensive Review on Transformers in Natural Language Processing. Journal of Artificial Intelligence Research, 70, 1-30.\n",
       "- Liu, Y., & Lapata, M. (2021). Text Summarization with Pretrained Encoders. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FINAL_WRITER_SYS_MSG = \"\"\"\n",
    "You take in a research report and a set of bullet points with feedback to improve,\n",
    "and you revise the research report based on the feedback and write a final version.\n",
    "\"\"\"\n",
    "\n",
    "prompt_final_writer = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', FINAL_WRITER_SYS_MSG),\n",
    "        ('human', 'Write a reviewed and improved version of this research report:\\n\\n{report}, based on this feedback:\\n\\n{feedback}.')\n",
    "    ]\n",
    ")\n",
    "llm_final_writer = ChatOpenAI(model='gpt-4o-mini', temperature=0.2)\n",
    "chain_final_writer = prompt_final_writer | llm_final_writer | output_parser\n",
    "\n",
    "output_final_report = chain_final_writer.invoke({'report': output, 'feedback': feedback_output})\n",
    "\n",
    "Markdown(output_final_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
