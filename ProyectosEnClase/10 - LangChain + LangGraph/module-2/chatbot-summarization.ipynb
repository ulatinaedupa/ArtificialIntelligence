{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fcadf3",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-2/chatbot-summarization.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239436-lesson-5-chatbot-w-summarizing-messages-and-memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651ead9-5504-45ee-938d-f91ac78dddd1",
   "metadata": {},
   "source": [
    "# Chatbot with message summarization\n",
    "\n",
    "## Review\n",
    "\n",
    "We've covered how to customize graph state schema and reducer. \n",
    " \n",
    "We've also shown a number of ways to trim or filter messages in graph state. \n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, let's take it one step further! \n",
    "\n",
    "Rather than just trimming or filtering messages, we'll show how to use LLMs to produce a running summary of the conversation.\n",
    " \n",
    "This allows us to retain a compressed representation of the full conversation, rather than just removing it with trimming or filtering.\n",
    "\n",
    "We'll incorporate this summarization into a simple Chatbot.  \n",
    "\n",
    "And we'll equip that Chatbot with memory, supporting long-running conversations without incurring high token cost / latency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_core langgraph langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09201a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# def _set_env(var: str):\n",
    "#     if not os.environ.get(var):\n",
    "#         os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# _set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464856d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-4o\",temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b",
   "metadata": {},
   "source": [
    "We'll use `MessagesState`, as before.\n",
    "\n",
    "In addition to the built-in `messages` key, we'll now include a custom key (`summary`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948e60f0-5c76-4235-b40e-cf523205d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "class State(MessagesState):\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855ea31-5cc1-4277-a189-0b72459f67ec",
   "metadata": {},
   "source": [
    "We'll define a node to call our LLM that incorporates a summary, if it exists, into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State):\n",
    "    \n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        \n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450",
   "metadata": {},
   "source": [
    "We'll define a node to produce a summary.\n",
    "\n",
    "Note, here we'll use `RemoveMessage` to filter our state after we've produced the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    \n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt \n",
    "    if summary:\n",
    "        \n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982993e-f4be-4ff7-9a38-886f75398b3d",
   "metadata": {},
   "source": [
    "We'll add a conditional edge to determine whether to produce a summary based on the conversation length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b507665d-7f5d-442a-b498-218c94c5dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State):\n",
    "    \n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    \n",
    "    # Otherwise we can just end\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd",
   "metadata": {},
   "source": [
    "## Adding memory\n",
    "\n",
    "Recall that [state is transient](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220) to a single graph execution.\n",
    "\n",
    "This limits our ability to have multi-turn conversations with interruptions. \n",
    "\n",
    "As introduced at the end of Module 1, we can use [persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence/) to address this! \n",
    " \n",
    "LangGraph can use a checkpointer to automatically save the graph state after each step.\n",
    "\n",
    "This built-in persistence layer gives us memory, allowing LangGraph to pick up from the last state update. \n",
    "\n",
    "As we previously showed, one of the easiest to work with is `MemorySaver`, an in-memory key-value store for Graph state.\n",
    "\n",
    "All we need to do is compile the graph with a checkpointer, and our graph has memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9",
   "metadata": {},
   "source": [
    "## Threads\n",
    "\n",
    "The checkpointer saves the state at each step as a checkpoint.\n",
    "\n",
    "These saved checkpoints can be grouped into a `thread` of conversation.\n",
    "\n",
    "Think about Slack as an analog: different channels carry different conversations.\n",
    "\n",
    "Threads are like Slack channels, capturing grouped collections of state (e.g., conversation).\n",
    "\n",
    "Below, we use `configurable` to set a thread ID.\n",
    "\n",
    "![state.jpg](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbadf3b379c2ee621adfd1_chatbot-summarization1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Lance! How can I assist you today?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned that your name is Lance. How can I help you today?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's great! The San Francisco 49ers have a rich history and a passionate fan base. Do you have a favorite player or a memorable game that you particularly enjoyed?\n"
     ]
    }
   ],
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"what's my name?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"i like the 49ers!\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2",
   "metadata": {},
   "source": [
    "Now, we don't yet have a summary of the state because we still have < = 6 messages.\n",
    "\n",
    "This was set in `should_continue`. \n",
    "\n",
    "```\n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "```\n",
    "\n",
    "We can pick up the conversation because we have the thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a93e9-f716-4980-8edf-94115017d865",
   "metadata": {},
   "source": [
    "The `config` with thread ID allows us to proceed from the previously logged state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, as of September 2023, Nick Bosa became the highest-paid defensive player in NFL history. He signed a five-year contract extension with the San Francisco 49ers worth $170 million, with $122.5 million guaranteed. Bosa is a key player for the 49ers, known for his exceptional skills as a defensive end.\n"
     ]
    }
   ],
   "source": [
    "input_message = HumanMessage(content=\"i like Nick Bosa, isn't he the highest paid defensive player?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lance introduced himself and expressed his support for the San Francisco 49ers, mentioning a particular liking for Nick Bosa. The conversation highlighted that Nick Bosa became the highest-paid defensive player in NFL history as of September 2023, with a five-year, $170 million contract extension with the 49ers.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "\n",
    "Let's review the trace!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiagents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
